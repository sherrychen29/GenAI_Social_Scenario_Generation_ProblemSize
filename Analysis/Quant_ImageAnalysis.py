#pip install opencv-python
import cv2 as cv
import csv
import numpy as np
import skimage
from skimage.measure import shannon_entropy
import pandas as pd
import os
from statsmodels.stats.proportion import proportion_confint
import matplotlib.pyplot as plt

# Quantative Image Analysis for DallE3 and GPTimage
# This script analyzes the quality of images generated by DallE3 and GPTimage for different
# problem sizes (bummer, glitch, disaster) using various metrics such as sharpness,
# contrast, entropy, resolution, and BRISQUE score.
# It calculates the probability that GPTimage is better than DallE3 for each metric and
# visualizes the results with confidence intervals.

scoreset=[]
#problem_size='bummer' #change this to the scenario you want to analyze, e.g., 'bummer', 'happy', 'sad', 'angry', 'confused', 'excited', 'scared', 'surprised'

#img1 = cv.imread("scenario_bummer_1_DallE3.png")
#img2 = cv.imread("scenario_bummer_1_GPTimage.png")
N=100
collect_stats=[]
for n in range(1, N+1, 10):
    print(f"Processing scenario size: {n}")
    scoreset = []
    for problem_size in ['bummer', 'glitch', 'disaster']:
        inputfolder=os.path.join(os.getcwd(),f"{problem_size.capitalize()}Folder")
        for s in range(1, n+1):
            for imagetype in ['DallE3', 'GPTimage']:
                
                img = cv.imread(os.path.join(inputfolder,f"scenario_{problem_size}_{s}_{imagetype}.png"))
                #Grayscale conversion is a common preprocessing step, as it simplifies image data while preserving essential information.
                gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
                #Laplacian Variance: A common method to assess sharpness is by calculating the variance of the Laplacian of an image. 
                # Higher variance values generally indicate sharper images with more detail.
                #The Laplacian method can also be used to detect blurriness in an image. A sharp image will have many edges, which leads to a higher variance in the Laplacian values across the image. Conversely, a blurry image will have fewer distinct edges, resulting in a lower variance of the Laplacian.
                #By calculating the variance of the Laplacian of an image, you can quantify its sharpness. If the variance is below a certain threshold, the image is considered blurry.
                #Blur Measurement (Laplacian Variance):

                Sharpness = cv.Laplacian(gray, cv.CV_64F).var() #high is better
                #Sharpness:
                #Refers to the clarity of edges and fine details in an image. High sharpness makes edges appear crisp and well-defined. 
                #Contrast:
                #Refers to the difference between the lightest and darkest areas of an image. High contrast can make an image appear more vibrant and punchy, while low contrast can make it look flat.

                #Contrast Measurement (Standard Deviation):
                #Contrast can be estimated by calculating the standard deviation of pixel intensities in a grayscale image. 
                # Higher standard deviation generally indicates higher contrast.
                Contrast = np.std(gray)

                #Image Quality Assessment (IQA):
                #BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator) is a no-reference image quality assessment algorithm that evaluates the perceptual quality of images without requiring a reference image.
                #It uses features extracted from the image to predict its quality score.
                #In OpenCV, the `cv.quality.QualityBRISQUE_compute` function computes the BRISQUE score for an image. 
                #This function requires a pre-trained model and range files to evaluate the image quality.
                #The BRISQUE score is a single value that indicates the quality of the image, where lower scores generally indicate better quality.
                #Note:
                #The BRISQUE algorithm is designed to evaluate the perceptual quality of images, and it is particularly useful for assessing images that may have distortions or artifacts. 
                #It is widely used in image processing and computer vision applications to evaluate the quality of images without needing a pristine reference image.
                # It requires pre-trained model and range files.
                BRISQUEscore = cv.quality.QualityBRISQUE_compute(img, "brisque_model_live.yml", "brisque_range_live.yml")[0]#lower is better
                
                #Noise Estimation:
                #.
                #Entropy was introduced by Shanon (1948), were the higher value of Entropy = more detailed information. 
                # Entropy is a measure of image information content, which is interpreted as the average uncertainty of information source.
                # In Image, Entropy is defined as corresponding states of intensity level which individual pixels can adapt. 
                # It is used in the quantitative analysis and evaluation image details, the entropy value is used as it provides better 
                # comparison of the image details..
                Entropy = shannon_entropy(gray)
                #Exposure Assessment:
                #Exposure levels (over or underexposure) can be estimated by analyzing the mean and variance of grayscale pixel values.
                mean_intensity = np.mean(gray)
                variance_intensity = np.var(gray)
                
                # Resolution (using Sobel operator)
                sobel_x = cv.Sobel(gray, cv.CV_64F, 1, 0, ksize=5)
                sobel_y = cv.Sobel(gray, cv.CV_64F, 0, 1, ksize=5)
                _obel = np.sqrt(sobel_x**2 + sobel_y**2)
                Resolution = np.mean(_obel)

                # Append the scores to the scoreset
                scoreset.append((problem_size,s, imagetype, Sharpness, Contrast, Entropy, Resolution, BRISQUEscore))
    scoreset_df = pd.DataFrame(scoreset, columns=['problem_size', 'Scenario', 'Image Type', 'Sharpness', 'Contrast', 'Entropy', 'Resolution', 'BRISQUE'])
    #scoreset_df.to_csv('ImageScore.csv', index=False)
    # Calculate mean for each numeric column, grouped by 'Image Type'

    #mean_scores = scoreset_df.groupby('Image Type').mean(numeric_only=True)
    #Upperbound_scores = scoreset_df.groupby('Image Type').quantile(0.995)
    #Lowerbound_scores = scoreset_df.groupby('Image Type').quantile(0.005)

    #print("Average scores by Image Type:")
    #print(mean_scores)

 
    from scipy.stats import mannwhitneyu, wilcoxon

    a=scoreset_df[scoreset_df['Image Type']=='GPTimage'][['Sharpness', 'Contrast', 'Entropy', 'Resolution', 'BRISQUE']]
    b=scoreset_df[scoreset_df['Image Type']=='DallE3'][['Sharpness', 'Contrast', 'Entropy', 'Resolution', 'BRISQUE']]
    b['BRISQUE'] = -b['BRISQUE']  # Invert BRISQUE for statistical test in batch
    #b['Contrast'] = -b['Contrast']  # Invert Contrast for statistical test in batch
    #b['Resolution'] = -b['Resolution']  # Invert Resolution for statistical test in batch
    stat, p_value = wilcoxon(a, b, alternative='less')
    print(f"Wilcoxon signed-rank test statistic: {stat}, p-value: {p_value}")


    # # Align DataFrames by 'Scenario' before subtraction
    # gptimage_aligned = gptimage_df[listofvars].set_index('Scenario')
    # dallle3_aligned = dallle3_df[listofvars].set_index('Scenario')
    # diff_df = gptimage_aligned- dallle3_aligned
    # diff_df = diff_df.set_index(gptimage_aligned.index)

    # Calculate the differences between GPTimage and DallE3 for each metric
    dalle3_Sharpness_df = scoreset_df[scoreset_df['Image Type']=='DallE3']['Sharpness']
    gptimage_Sharpness_df = scoreset_df[scoreset_df['Image Type']=='GPTimage']['Sharpness']
    diff_Sharpness = gptimage_Sharpness_df.values - dalle3_Sharpness_df.values

    dalle3_Contrast_df = scoreset_df[scoreset_df['Image Type']=='DallE3']['Contrast']
    gptimage_Contrast_df = scoreset_df[scoreset_df['Image Type']=='GPTimage']['Contrast']
    diff_Contrast = gptimage_Contrast_df.values - dalle3_Contrast_df.values

    dalle3_entropy_df = scoreset_df[scoreset_df['Image Type']=='DallE3']['Entropy']
    gptimage_entropy_df = scoreset_df[scoreset_df['Image Type']=='GPTimage']['Entropy']
    diff_Entropy = gptimage_entropy_df.values - dalle3_entropy_df.values

    dalle3_resolution_df = scoreset_df[scoreset_df['Image Type']=='DallE3']['Resolution']
    gptimage_resolution_df = scoreset_df[scoreset_df['Image Type']=='GPTimage']['Resolution']
    diff_Resolution = gptimage_resolution_df.values - dalle3_resolution_df.values

    dalle3_brisque_df = scoreset_df[scoreset_df['Image Type']=='DallE3']['BRISQUE']
    gptimage_brisque_df = scoreset_df[scoreset_df['Image Type']=='GPTimage']['BRISQUE']
    diff_BRISQUE = gptimage_brisque_df.values - dalle3_brisque_df.values; 


    # Calculate the confidence intervals of the probability that GPTimage is better than DallE3

    def win_confidence_interval(diff_series, x, alpha=0.05):
        wins = (diff_series > 0).sum()
        prob = wins / x
        ci_low, ci_upp = proportion_confint(wins, x, alpha=alpha, method='wilson')
        return prob, (ci_low, ci_upp)

    # Sharpness
    prob_sharp, ci_sharp = win_confidence_interval(diff_Sharpness, n*3)
    #print(f'Probability GPTimage > DallE3 (Sharpness): {prob_sharp:.3f}, 95% CI: {ci_sharp}')

    # Contrast
    prob_contrast, ci_contrast = win_confidence_interval(diff_Contrast, n*3)
    #print(f'Probability GPTimage > DallE3 (Contrast): {prob_contrast:.3f}, 95% CI: {ci_contrast}')

    # Entropy
    prob_entropy, ci_entropy = win_confidence_interval(diff_Entropy, n*3)
    #print(f'Probability GPTimage > DallE3 (Entropy): {prob_entropy:.3f}, 95% CI: {ci_entropy}')

    # Resolution
    prob_resolution, ci_resolution = win_confidence_interval(diff_Resolution, n*3)
    #print(f'Probability GPTimage > DallE3 (Resolution): {prob_resolution:.3f}, 95% CI: {ci_resolution}')

    prob_brisque, ci_brisque = win_confidence_interval(-diff_BRISQUE, n*3)
    #print(f'Probability GPTimage < DallE3 (BRISQUE): {prob_brisque:.3f}, 95% CI: {ci_brisque}')

    collect_stats.append({
        'Scenario': n*3,
        'Sharpness': prob_sharp,
        'Contrast': prob_contrast,
        'Entropy': prob_entropy,
        'Resolution': prob_resolution,
        'BRISQUE': prob_brisque,
        'Sharpness LB': ci_sharp[0],
        'Sharpness UB': ci_sharp[1],
        'Contrast LB': ci_contrast[0],
        'Contrast UB': ci_contrast[1],
        'Entropy LB': ci_entropy[0],
        'Entropy UB': ci_entropy[1],
        'Resolution LB': ci_resolution[0],
        'Resolution UB': ci_resolution[1],
        'BRISQUE LB': ci_brisque[0],
        'BRISQUE UB': ci_brisque[1]
    })
# Convert the collected stats to a DataFrame and save it
stats_df = pd.DataFrame(collect_stats)

outputfolder = os.path.join(os.getcwd(), f"StatsResults")
stats_df.to_csv(os.path.join(outputfolder, f"ImageAnalysis_Stats_Quant.csv"), index=False)



metrics = ['Sharpness','Entropy', 'Resolution', 'BRISQUE']
plt.figure(figsize=(15, 8))

for i, metric in enumerate(metrics, 1):
    plt.subplot(2, 2, i)
    x=stats_df['Scenario']
    plt.plot(x, stats_df[metric], label=f'Probability GPT4o is better than Dall-E3 ({metric})')
    plt.fill_between(x, stats_df[f'{metric} LB'], stats_df[f'{metric} UB'], alpha=0.2, label='95% Confidence Interval')
    plt.xlabel('Number of Scenario')
    plt.ylabel('Probability')
    plt.xlim(150, 300)
    plt.title(metric)
    plt.legend()

plt.tight_layout()

# Save the plot

#plt.savefig(os.path.join(outputfolder, f"ImageAnalysis_Probabilities_combined_high_95%.png"))
plt.show()

